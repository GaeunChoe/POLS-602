---
title: "Problem Set 4"
author: "Ellie Choe"
date: "2025-12-12"
output: pdf_document
---

# Part 1: Reading
## 1. What is the di>erence between a confounder and a collider? How should you address each in your models?
A collider is influenced by both the exposure (X) and the outcome (Y), whereas a confounder influences both X and Y. We should not control for a collider because doing so induces bias in the estimate of the effect of X on Y. Conversely, we must control for a confounder; failing to do so will yield a biased result.

## 2. How can conditioning on a collider create bias?
Conditioning on a collider creates a spurious association between two variables that are otherwise independent. It opens a non-causal path between the exposure and the outcome, forcing a relationship where none exists or distorting the true relationship.

## 3. Why can’t statistical summaries or correlations alone tell us whether to control for a variable?
Statistical summaries and visualizations can be identical for datasets generated by completely different causal mechanisms. For instance, the correlation between X and Z can be the same regardless of whether Z is a confounder or a collider. Statistics alone cannot express the directionality of causal relationships, which is required to identify potential bias.

## 4. What is meant by a “kitchen sink” regression, and what is wrong with this approach to modeling?
Kitchen sink regression refers to a multivariable regression procedure where all available variables—regardless of their association with the outcome—are entered into a model, followed by an automated or manual variable selection strategy based on p-values or model-based information criteria. This approach is problematic because it ignores the directionality of relationships, produces effect estimates with no meaningful causal interpretation, increases the alpha error rate due to multiple testing, leads to overfitting and model instability, and disregards common sense or domain expertise.

## 5. What is a “backdoor path” and how does multiple regression help block these paths?
A backdoor path creates a non-causal association between the exposure and the outcome, even if there is no direct association between them. Multiple regression helps block these paths by conditioning on (or adjusting for) the variables that lie on the path, effectively holding them constant to isolate the true causal effect.

# Part 2: Simulation
```{r}
set.seed(111)
n <- 2000

# Generate random data for variables that are not causally a>ected by any others 
C <- rnorm(n, 0, 1) # confounder: war
I <- rbinom(n, 1, 0.5) # instrument: EU refugee intake policy
U <- rnorm(n, 0, 1) # exogenous variable: local economic hardship

# Treatment X: refugee numbers
alpha0 <- 0
alpha_C_on_X <- 0.6 # war -> refugee numbers
alpha_I_on_X <- 0.9 # EU policy _> refugee numbers
errorterm_X <- rnorm(n, 0, 1) 
X <- alpha0 + alpha_C_on_X * C + alpha_I_on_X * I + errorterm_X

# Mediator M: perceived security threat
beta_M_intercept <- 0
beta_X_on_M <- 0.8
beta_C_on_M <- 0.5
errorterm_M <- rnorm(n, 0, 1)
M <- beta_M_intercept + beta_X_on_M * X + beta_C_on_M * C + errorterm_M

# Outcome Y: far-right party support
beta0 <- 0
beta_X_on_Y <- 1.0
beta_M_on_Y <- 0.6
beta_C_on_Y <- 0.7
beta_U_on_Y <- 0.5
errorterm_Y <- rnorm(n, 0, 1)
Y <- beta0 + beta_X_on_Y * X + beta_M_on_Y * M + beta_C_on_Y * C + beta_U_on_Y * U + errorterm_Y

# Collider S: immigration policy support
gamma_X_on_S <- 0.7
gamma_Y_on_S <- 0.5
errorterm_S <- rnorm(n, 0, 1)
S <- gamma_X_on_S * X + gamma_Y_on_S * Y + errorterm_S

# Combine into a data frame
df <- data.frame(Y, X, M, C, I, U, S)
head(df)

# 1. estimate X's direct effect
mod_direct <- lm(Y ~ X + M + C + U, data = df)
summary(mod_direct)

# 2. estimate X's total effect
mod_total <- lm(Y ~ X + C + U, data = df)
summary(mod_total)

# 3. Control for the collider, the exogenous independent variable, or the instrument. 
mod_collider <- lm(Y ~ X + C + U + S, data = df)
summary(mod_collider)

mod_noU <- lm(Y ~ X + C, data = df)
summary(mod_noU)

mod_instrument <- lm(Y ~ X + C + U + I, data = df)
summary(mod_instrument)
```
Controlling for the exogenous variable ($U$) did not meaningfully change the coefficient for $X$ (1.477 vs. 1.500), confirming that omitting $U$ does not introduce bias. However, including $U$ reduced the standard error of the estimate (from 0.026 to 0.024). This indicates that while exogenous variables are not necessary for identification, they improve model efficiency by reducing unexplained variance.

4. To estimate the total effect of refugee numbers (X) on far-right party support (Y), we include confounders (C) and exogenous variables affecting Y (U) but exclude the mediator (M) and the collider (S). The regression results show that X has a coefficient of 1.477, indicating that it increases Y both directly and indirectly through M. Controlling for confounders and exogenous predictors improves estimate precision without blocking causal pathways. Including the collider reduces the X coefficient, introducing bias, while including the instrument does not meaningfully change it.
